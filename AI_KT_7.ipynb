{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUgesoeiz5P8"
      },
      "source": [
        "# Tehisintellekti 7. kodutöö - linnuliigi ennustamine heliklipi alusel\n",
        "## Baaseerub Kaggle'i võistlusel BirdCLEF 2024\n",
        "### Autorid: Charleen Konsa, Timo Kaasik ja Edvard Notberg\n",
        "\n",
        "Seitsmenda kodutöö raames lahendasime Kaggle'i võistluse BirdCLEF 2024 lihtsustatud versiooni. Võistluse info asub lingil https://www.kaggle.com/competitions/birdclef-2024. Kõik kasutatud andmed on võetud võistluse lehelt. Kuna võistluse koguandmestik oli väga mahukas (182 linnuliiki, heliklippe 7+ GB mahus), siis on kasutatud ainult osa kõigist andmetest. Meie kasutatud andmed on üles laetud GitHubi repositooriumisse (link - https://github.com/Ckonsa/AI_KT_7), et oleks ühtselt mõistetav, mis andmeid ja mis mahus on töös kasutatud.\n",
        "\n",
        "Töö eesmärk on kasutada masinõpet, et tuvastada heliklipi abil India linnuliike. Etteantud klipi puhul klassifitseeritakse, millisele linnuliigile klipis kõlav heli kuulub."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vajalikud installeerimised ja impordid"
      ],
      "metadata": {
        "id": "ubrsQxrMdurI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vajalikud installimised\n",
        "!pip install tensorflow-io\n",
        "!pip install tensorflow --upgrade"
      ],
      "metadata": {
        "id": "1h3a_Y5ndjQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH_05zuO48BV"
      },
      "outputs": [],
      "source": [
        "# Impordid\n",
        "import pandas as pd\n",
        "import ast\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import Audio\n",
        "import random\n",
        "import numpy as np\n",
        "import glob\n",
        "# Colabi jaoks\n",
        "import os\n",
        "from google.colab import drive\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I20JJC58gsyt",
        "outputId": "06ac675f-45b8-4a2c-fc0d-054a4917e75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Colabi mount\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTZgtTzZ4H32"
      },
      "source": [
        "### Andmete analüüs\n",
        "Esmalt laeme alla informatisooni failist train_metadata.csv. Selles hoiustatakse informatsiooni kõigi treeninghelikilppide kohta. Uurime ja analüüsime täpsemalt etteantud klippide informatsiooni, et paremini mõista, milliste andmetega tegutseme.\n",
        "\n",
        "NB! Analüüsis on kaasatud kõigi treeningandmete metaandmed. Mudelite juures on kasutatud ainult väikest osa kõigist andmetest. Seega kõigi andmete pealt tehtud järeldused ei kandu üle tegelikult kasutatud andmetele."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "HumKu7-GzxRY",
        "outputId": "5055adb6-62e7-4d32-f9e4-081422b70f03"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-43aac07ec252>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_metadata.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Informatsiooni andmestiku suurus on {metadata.shape}. Seega on meil esialgu {metadata.shape[0]} heliklippi.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Andmestikus on olemas read {list(metadata.columns)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "metadata = pd.read_csv(\"train_metadata.csv\")\n",
        "print(f\"Informatsiooni andmestiku suurus on {metadata.shape}. Seega on meil esialgu {metadata.shape[0]} heliklippi.\")\n",
        "print(f\"Andmestikus on olemas read {list(metadata.columns)}.\")\n",
        "metadata.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV3H7vqK6zan"
      },
      "source": [
        "Heliklippide informatsioon on jaotatud 12-sse tulpa. Kõigi tulpade informatsiooni pole aga tarvis seega on mõislik andmestikku eelnevalt korrastada. Lisaks on mõnes tulbas (latitude ja longitude) osa informatsioonist puudu, mis tuleb enne sobival viisil täita.\n",
        "\n",
        "Eemaldatavate tulpade hulgas on järgnevad:\n",
        "\n",
        "\n",
        "*   secondary_label\n",
        "*   scientific_name\n",
        "*   common_name\n",
        "*   author\n",
        "*   license\n",
        "*   url\n",
        "\n",
        "Treenimisel hakatakse ennustama tulpa primary_label, kus on linnu kohta käiv märgend. Kuna linnu märgend on olemas, siis pole masinõppe jaoks tarvis teada linnu teaduslikku ega inimkeelset nime. Samuti on muu informatsioon autori, litsentsi kui ka heliklipi URLi kohta ebavajalik, sest neid ei saa treenimisel kasutada.\n",
        "\n",
        "Reitingut on võimalik kasutada, et kaasata treenimisele ainult heliklippe, mille kvaliteet on parem. Sel juhul antakse mudelile selgema linnulauluga klipid ja selle alusel on lihtsam õppida lindude hääli."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajsSjCovz41m"
      },
      "outputs": [],
      "source": [
        "# Ebavajalike veergude eemldamine\n",
        "metadata = metadata.drop([\"secondary_labels\", \"scientific_name\", \"common_name\", \"author\", \"license\", \"url\"], axis = 1)\n",
        "# Parema reitinguga heliklippide valimine\n",
        "threshold = 3.0\n",
        "metadata = metadata[metadata[\"rating\"] >= threshold]\n",
        "print(f\"Informatsiooni andmestiku suurus on nüüd {metadata.shape}.\")\n",
        "# Enam pole rating veeru vaja ehk eemaldame ka selle\n",
        "metadata = metadata.drop([\"rating\"], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNN1b36I_NwT"
      },
      "outputs": [],
      "source": [
        "# Latitude ja longitude Nan väärtuste väärtustamine\n",
        "# Asendusväärtusteks võetakse keskmised väärtused\n",
        "metadata[\"latitude\"].fillna(metadata[\"latitude\"].mean(), inplace=True)\n",
        "metadata[\"longitude\"].fillna(metadata[\"longitude\"].mean(), inplace=True)\n",
        "metadata.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWZnmXPVEe6e"
      },
      "outputs": [],
      "source": [
        "# Veeru type listiks muutmine\n",
        "metadata[\"type\"] = metadata[\"type\"].apply(ast.literal_eval)\n",
        "# Leiame, mis on 10 enimlevinud type väärtused ja viima iga väärtuse veeru kujule\n",
        "type_count = Counter([item.lower() for sublist in metadata[\"type\"] for item in sublist])\n",
        "types = list(dict(sorted(type_count.items(), key=lambda item: item[1], reverse=True)).keys())\n",
        "# Eemaldame tühja tüübi ja valime 10 kõige sagedasemat tüüpi\n",
        "types.remove(\"\")\n",
        "frequent_types = types[:10]\n",
        "# Iga tüübi jaoks luuakse veerg. Veerus on 1 kui see tüüp oli esialgses listis, muul juhul on 0\n",
        "for frequent_type in frequent_types:\n",
        "    metadata[frequent_type] = metadata[\"type\"].apply(lambda x: 1 if frequent_type in [item.lower() for item in x] else 0)\n",
        "# Eemaldame esialge type veeru\n",
        "metadata = metadata.drop([\"type\"], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw_sewJrBb0u"
      },
      "source": [
        "Edasi analüüsin alles jäänud heliklippide informatsiooni, et näha kas andmed on tasakaalus ja millised andmetega on täpselt tegu. Kuna allesjäänud veerge on vähe, siis pole keeruline neisse süüvida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke-1mWhaLIsf"
      },
      "outputs": [],
      "source": [
        "# Treeningandmete tasakaalukus linnuliikide alusel\n",
        "bird_types = metadata[\"primary_label\"].value_counts()\n",
        "print(f\"Erinevaid linnuliike on kokku {len(bird_types)}.\")\n",
        "\n",
        "bird_types.plot(kind=\"bar\")\n",
        "plt.title(\"Heliklippide arv iga linnuliigi kohta\")\n",
        "plt.xlabel(\"Linnuliigid\")\n",
        "plt.ylabel(\"Heliklippide arv\")\n",
        "plt.xticks([]) # Muidu proovitakse kõigi linnuliikide märgendeid kuvada\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzgCSLmBNGZM"
      },
      "outputs": [],
      "source": [
        "# Andmestikus olevad pikkus- ja laiuskraadi ning nende sõltuvus linnuliigist\n",
        "sns.scatterplot(data=metadata, x=\"latitude\", y=\"longitude\", hue=\"primary_label\", legend=False)\n",
        "plt.title(\"Lindude laius- ja pikkuskraadid\")\n",
        "plt.xlabel(\"Laiuskraad\")\n",
        "plt.ylabel(\"Pikkuskraad\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxb1R7vdRac8"
      },
      "outputs": [],
      "source": [
        "# Erinevate linnuhäälte tüüpide kogus andmestikus\n",
        "types_value_counts = []\n",
        "for frequent_type in frequent_types:\n",
        "  types_value_counts.append(metadata[frequent_type].value_counts()[1])\n",
        "empty_types = (metadata[frequent_types] == 0).all(axis=1).sum()\n",
        "frequent_types.append(\"NO TYPE\")\n",
        "types_value_counts.append(empty_types)\n",
        "plt.bar(frequent_types, types_value_counts)\n",
        "plt.title(\"Linnuhäälte tüübid\")\n",
        "plt.xlabel(\"Tüüp\")\n",
        "plt.ylabel(\"Kogus\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LV_0kP_Z0s0"
      },
      "source": [
        "Esimesel graafikul uuritakse heliklippide kogust ennustatavate liikide kohta. Sellelt on näha, et liigid, mida soovitakse ennustada pole eriti tasakaalus. On liike, millel on heliklippe üle 400, kuid samal ajal on osaldel liikidel kõigest 5 klippi. See võib põhjustada edaspidi probleeme, sest mudel ei pruugi olla võimeline liike ainult viie klipi pealt ennustama. Seetõttu on võib olla mõeldav osade liikide mudelist välja jätmine ja ainult nende linnuliikide ennustamine, millel on klippe rohkem.\n",
        "\n",
        "Teisest graafikust tulevad välja seosed liikide ja pikkus- ning laiuskraadide vahel. On näha, kuidas heliklippe tuleb erinevates paikadest, kust tulevad välja ka peamised seal pesitsevad linnuliigid. Osa linnuliike leidub mitmes eri paigas. See võib mõjutada nende lindude häält, sest nagu inimestel on ka osadel linnuliikidelgi dialektid.\n",
        "\n",
        "Kolmandast graafikust on näha, millised on peamised linnuheli tüübid. Tüüpidest on alles jäetud ainult 10 kõige sagedasemat. Kõige sagedasemad on tüübi \"call\" ja \"snog\". Samas on siiski ka päris palju heliklippe, millel ei olnud üldse tüüpi või mille tüüp ei kuulu 10 sagedaseima hulka. Seega oleks seda atribuuti väga keeruline edaspidi kaasata, sest enamikel juhtudel see informatsioon on puudu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfMCkqg1g_83"
      },
      "source": [
        "### Mudel 1 - spektogrammid\n",
        "\n",
        "Mudel baseerub Tensorflow õpetusel nimega “Audio Data Preparation and Augmentation” (link õpetusele - https://www.tensorflow.org/io/tutorials/audio)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Andmete eeltöötlus"
      ],
      "metadata": {
        "id": "t4M_9dpkjBau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YgH4ayDR7rYn"
      },
      "outputs": [],
      "source": [
        "# Esimese mudeli jaoks vajalik eeltöötlus.\n",
        "# Võtab argumentideks tee helifailini ja helifaili labeli\n",
        "# Tagastab heliklipist tehtud spektogrammi ja labeli\n",
        "def preprocess(file_path, label):\n",
        "    audio = tfio.audio.AudioIOTensor(file_path, dtype=tf.int16)\n",
        "    # audio_slice on praegu jäetud TF dokumentatsiooni näitest\n",
        "    audio_slice = audio[100:]\n",
        "    # Teeb 1-D tensoriks\n",
        "    audio_tensor = tf.squeeze(audio_slice, axis=[-1])\n",
        "    # Normaliseerib\n",
        "    tensor = tf.cast(audio_tensor, tf.float32) / 32768.0\n",
        "    # Võtab kindla pikkusega osa, et iga klipp oleks sama pikkusega\n",
        "    tensor = tensor[:48000]\n",
        "    zero_padding = tf.zeros([48000] - tf.shape(tensor), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, tensor],0)\n",
        "    # Loob spektogrammi\n",
        "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    return spectrogram, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kõigi failide sisse lugemine ja labeli numbri lisamine\n",
        "# Oleme kasutanud 10 linnuliiki\n",
        "liigid = [\"commoo3\", \"gargan\", \"kenplo1\", \"nutman\", \"pursun4\", \"zitcis\", \"wemhar1\", \"woosan\", \"asbfly\", \"ashdro1\"]\n",
        "files = []\n",
        "# Eeldatakse, et heliklipid asuvad liiginimelises kaustas\n",
        "for i in range(len(liigid)):\n",
        "  liik = liigid[i]\n",
        "  # Leitakse kõigi heliklippide pathid\n",
        "  liigi_failid = glob.glob(liik+\"/*.ogg\")\n",
        "  for f in liigi_failid:\n",
        "    files.append((f, i))"
      ],
      "metadata": {
        "id": "YdGzPerNeFAb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a80cb3f0-fe28-4ab3-8dc0-227be6cb7dd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'glob' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d08afbb32406>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mliik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliigid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Leitakse kõigi heliklippide pathid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mliigi_failid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliik\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/*.ogg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mliigi_failid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proovime eeltöötluse läbi ühe heliklipiga\n",
        "s, l = preprocess(files[1][0], files[1][1])\n",
        "plt.figure(figsize=(30,20))\n",
        "plt.imshow(tf.transpose(s)[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SHOf5O5DeSKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teeme eeltöötluse kõigile olemasolevatele andmetele\n",
        "X = []\n",
        "y = []\n",
        "for file_path, label in files:\n",
        "  s, l = preprocess(file_path, label)\n",
        "  X.append(s)\n",
        "  y.append(l)\n",
        "\n",
        "# Ennustatav atribuut ehk linnuliigid one-hot encodeitakse, et seda saaks mudelile ette anda\n",
        "y = to_categorical(y, num_classes=10)\n",
        "# Normaliseerimine ja reshapeime andmed\n",
        "X = X / np.max(np.abs(X))\n",
        "X = X[..., np.newaxis]\n",
        "# Jagame andmed treening- ja testandmeteks 80/20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Treeningandmed on mõõtmetega {X_train[0].shape}\")"
      ],
      "metadata": {
        "id": "enQ6ocJLexwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdS-wez7fVWa"
      },
      "source": [
        "#### Mudeli loomine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKLobzc5FtnJ"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(1491, 257, 1)),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "  \t])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_train, y_train, epochs = 20, batch_size = 32)"
      ],
      "metadata": {
        "id": "O3tFwkX3gqxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Mudeli 1 loss\")\n",
        "plt.plot(range(1, 21), hist.history[\"loss\"], \"r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pSiRoX8Ulq0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mudeli testimine"
      ],
      "metadata": {
        "id": "TKVyLkMvkUdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mudeli peal testandmete ennustamine\n",
        "y_pred = model.predict(X_test)\n",
        "# Vaatame, mis klassi kõige tõenäolisemalt ennustatakse\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "actual = np.argmax(y_test, axis=1)\n",
        "# Vaatame, mis mudel ennustas ja mis oli tegelikult õige\n",
        "print(f\"Testandmeid on {len(X_test)}.\")\n",
        "for i in range(len(y_pred)):\n",
        "    print(f\"Ennustus: {y_pred_classes[i]}, Tegelik: {actual[i]}\")\n",
        "# Leiame täpsuse testandmetel\n",
        "accuracy = accuracy_score(actual, y_pred_classes)\n",
        "print(\"Täpsus:\", accuracy)"
      ],
      "metadata": {
        "id": "bPv_GTuPsK9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mudeli 1 edasised katsetused\n",
        "Mudeli 1 edasikatsetusena proovime jaotada ühte heliklippi mitmeks klipiks. Proovime näha, kas sellel võiks olla positiivne mõju."
      ],
      "metadata": {
        "id": "i60Dg-Cry-Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laeb heliklipi, teeb audiotensoriks, resampleb 16 kHz monochanneliks\n",
        "# Võtab argumendiks tee failini\n",
        "# Tagastab modifitseeritud heliklipi\n",
        "def load_ogg_16k_mono(filename):\n",
        "    res = tfio.audio.AudioIOTensor(filename)\n",
        "    # Muudab tensoriks ja paneb channelid kokku\n",
        "    tensor = res.to_tensor()\n",
        "    tensor = tf.math.reduce_sum(tensor, axis=1) / 2\n",
        "    # Võtab sample ratei\n",
        "    sample_rate = res.rate\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    # Resampleb 16 kHz\n",
        "    ogg = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000)\n",
        "    return ogg"
      ],
      "metadata": {
        "id": "4S6fAenDy9Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktsiooni testimine ühel heliklipil\n",
        "klipi_nimi = \"woosan/XC25111.ogg\"\n",
        "klipp = load_ogg_16k_mono(klipi_nimi)"
      ],
      "metadata": {
        "id": "Tz6QAL5rzeRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eeltöötlus andmetele\n",
        "def preprocess_ogg(sample, index):\n",
        "    sample = sample[0]\n",
        "    zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32)\n",
        "    wav = tf.concat([zero_padding, sample],0)\n",
        "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
        "    return spectrogram"
      ],
      "metadata": {
        "id": "fB0k1f3m1eEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_slices = tf.keras.utils.timeseries_dataset_from_array(klipp, klipp, sequence_length=16000, sequence_stride=16000, batch_size=1)\n",
        "audio_slices = audio_slices.map(preprocess_ogg)\n",
        "audio_slices = audio_slices.batch(64)"
      ],
      "metadata": {
        "id": "8HmZo2UT1ohn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_2 = []\n",
        "y_2 = []\n",
        "\n",
        "liigid = [\"commoo3\", \"gargan\", \"kenplo1\", \"nutman\", \"pursun4\", \"zitcis\", \"wemhar1\", \"woosan\", \"asbfly\", \"ashdro1\"]\n",
        "files = []\n",
        "# for i in range(liigid[1]):\n",
        "liik = liigid[0]\n",
        "liigi_failid = glob.glob(liik+\"/*.ogg\")\n",
        "\n",
        "for f in liigi_failid[:10]:\n",
        "  klipp = load_ogg_16k_mono(f)\n",
        "\n",
        "  audio_slices = tf.keras.utils.timeseries_dataset_from_array(klipp, klipp, sequence_length=16000, sequence_stride=16000, batch_size=1)\n",
        "  audio_slices = audio_slices.map(preprocess_ogg)\n",
        "  audio_slices = audio_slices.batch(64)\n",
        "\n",
        "  for s in audio_slices:\n",
        "    for el in s:\n",
        "      X.append(el)\n",
        "      y.append(0)\n"
      ],
      "metadata": {
        "id": "XMqiDk_v2XJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mudel 1.2\n",
        "model = Sequential([\n",
        "    Input(shape=(1491, 257, 1)),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "  \t])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])"
      ],
      "metadata": {
        "id": "ip-UxHTO57kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_2, y_2, epochs = 4, batch_size = 32)"
      ],
      "metadata": {
        "id": "pZPNG7UX6Ct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Mudeli 1.2 loss\")\n",
        "plt.plot(range(1, 21), hist.history[\"loss\"], \"r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vUuHsDDknsBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mudel 2 - MFCCs\n",
        "\"Mel Frequency Cepstral Coefficents (MFCCs) is a way of extracting features from an audio.\"\n",
        "\n",
        "Audiote klassifitseerimisel on lisaks võimalik kasutada MFCCs. Kuna MFCCs väljund on teistsugune võrreldes spektogrammi omaga, on põneva näha kuidas mudelid omavahel erinevad ja milline neist paremini ennustab."
      ],
      "metadata": {
        "id": "R7b7dMaGCwsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eeltöötlus"
      ],
      "metadata": {
        "id": "kvdCKARVqAjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Failide eeltöötlus, võtav sisendiks järjendi, kus on ennikud, mille esimesel kohal on audiofaili tee ja teisel kohal label numbrina\n",
        "# Vaikeväärtustena on sr - Sample rate = 22050, n_mfcc = 13, seg_len - Segmenth length in seconds = 5\n",
        "\n",
        "# Kõike võib vastavalt vajadusele muuta, ühes kohas, funktsiooni päises\n",
        "# sr = 22050 või 44100?, ei tea kas 44100 läheks paljuks\n",
        "# n_mfcc = 13, kuid suurem peaks parem olema, ent mingisuguse maani\n",
        "# seg_len = 5, kuidas soovime\n",
        "def process_files(files, sr = 22050, n_mfcc=13, seg_len = 5):\n",
        "  # Kogume eraldi kokku MFCCs ja labelid\n",
        "  segment_mfccs = []\n",
        "  labels = []\n",
        "  # Käime läbi kõik etteantud failid\n",
        "  for file_path, label in files:\n",
        "    audio, sr = librosa.load(file_path, sr=sr)\n",
        "    # Võtame tervest audiost kindla pikkusega sämpli.\n",
        "    sample_segment_len = int(seg_len * sr)\n",
        "    #Kontrollime, et audio oleks ikka 5 sekundit pikk.\n",
        "    #Kui ei ole, paddime vaikusega (0-idega)\n",
        "    if len(audio) < sample_segment_len:\n",
        "      padding = sample_segment_len - len(audio)\n",
        "      audio = np.pad(audio, (0, padding), 'constant')\n",
        "\n",
        "    # 5-sekundiline klipp võetakse suvaliselt. Ei ole grantiid, et see klipp ei ole vaikus vms.\n",
        "    # Ideaalis võiks võtta klipi, milles on kõige enam infot\n",
        "    # Hea kasutada kui ei saa eraldada nii palju ajalist ressurssi\n",
        "    #start_sample = random.randint(0, (len(audio) - sample_segment_len))\n",
        "    #sample = audio[start_sample:(start_sample + sample_segment_len)]\n",
        "\n",
        "    # Valime heliklipist jupi potentsiaalselt \"suurima energiaga\"\n",
        "    # See võib failide arvu kasvades kulukaks tööks minna\n",
        "    if len(audio) > sample_segment_len:\n",
        "      segments = int(len(audio) / sample_segment_len)\n",
        "      energy = [np.sum(np.square(audio[i * sample_segment_len:(i + 1) * sample_segment_len])) for i in range(segments)]\n",
        "      valik = np.argmax(energy)\n",
        "      start_sample = valik * sample_segment_len\n",
        "      sample = audio[start_sample:start_sample + sample_segment_len]\n",
        "    else:\n",
        "      sample = audio\n",
        "\n",
        "    # Normaliseerimine\n",
        "    sample = librosa.util.normalize(sample)\n",
        "    # MFCCs võtmine.\n",
        "    mfccs = librosa.feature.mfcc(y=sample, sr=sr, n_mfcc=13)\n",
        "    segment_mfccs.append(mfccs)\n",
        "    labels.append(label)\n",
        "  return segment_mfccs, labels"
      ],
      "metadata": {
        "id": "mvCK12GLHNY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Võtame 10 linnuliiki ja igaühel 50 heliklippi\n",
        "liigid = ['asbfly', 'ashdro1', 'commoo3', 'gargan', 'kenplo1', 'nutman', 'pursun4', 'wemhar1', 'woosan', 'zitcis']"
      ],
      "metadata": {
        "id": "2U4QTMkMmEc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kõigi failide sisse lugemine ja labeli numbri lisamine\n",
        "files = []\n",
        "for i in range(len(liigid)):\n",
        "  liik = liigid[i]\n",
        "  liigi_failid = glob.glob(liik+\"/*.ogg\")\n",
        "  for f in liigi_failid:\n",
        "    files.append((f, i))\n",
        "# Ajan järjendis heliklipid segamine, et edaspidi oleks lihtsam treening ja testandmeteks jagada\n",
        "np.random.shuffle(files)"
      ],
      "metadata": {
        "id": "kcOfeAV6R0jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Andmete eeltöötlus\n",
        "X, y = process_files(files)\n",
        "#Klasside numbrid dünaamiliselt ja traintest suuruse määramine. Hetkel on testandmete suurus 20%\n",
        "num_of_classes = len(np.unique(y)) + 1\n",
        "traintest_size = 0.2\n",
        "# Ennustava atribuudi one-hot encodimine, et seda anda mudelile ette\n",
        "y = to_categorical(y, num_classes=num_of_classes)\n",
        "\n",
        "# Probleem: andmed on arrayde arrayd, flattenib iga array X-is, et sobiks standarscalerile\n",
        "# Teeb listi 2D arrayks\n",
        "X_flattened = np.array([item.flatten() for item in X])\n",
        "\n",
        "# StandardScaleme\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_flattened)\n",
        "\n",
        "# Rehsape tagasi\n",
        "feature_count = X[0].shape[0]\n",
        "time_steps = X[0].shape[1]\n",
        "X_scaled = X_scaled.reshape(len(X_scaled), feature_count, time_steps, 1)\n",
        "\n",
        "# Train test split\n",
        "test_len = int(len(X_scaled) * traintest_size)\n",
        "X_train = X_scaled[test_len:]\n",
        "y_train = y[test_len:]\n",
        "X_test = X_scaled[:test_len]\n",
        "y_test = y[:test_len]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lasslJQcrpu",
        "outputId": "ccc70815-e4ed-4551-9883-dca79071dbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 421ms/step - accuracy: 0.0956 - loss: 2.8398\n",
            "Epoch 2/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1065 - loss: 2.3926\n",
            "Epoch 3/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1040 - loss: 2.3978\n",
            "Epoch 4/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1026 - loss: 2.3425\n",
            "Epoch 5/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1360 - loss: 2.2999\n",
            "Epoch 6/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1142 - loss: 2.3292\n",
            "Epoch 7/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1452 - loss: 2.2493\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7add5d799c30>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mudeli loomine"
      ],
      "metadata": {
        "id": "Zui1zp_ZqIeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mudeli loomine\n",
        "model2 = Sequential([\n",
        "    Input(shape=(feature_count, time_steps, 1)),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "    Flatten(),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_of_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile ja fit\n",
        "model2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model2.summary()\n",
        "hist = model2.fit(X_train, y_train, epochs=20, batch_size=32)"
      ],
      "metadata": {
        "id": "J3cRvRSrqHZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Mudeli 2 loss\")\n",
        "plt.plot(range(1, 21), hist.history[\"loss\"], \"r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AhJNt7BSsm4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mudeli testimine"
      ],
      "metadata": {
        "id": "CS1GgnT5qUiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mudeli testimine testandmetel\n",
        "pred = model.predict(X_test)\n",
        "correct = 0\n",
        "\n",
        "print(f\"Testandmeid on {len(X_test)}.\")\n",
        "# Vaadatakse palju testandmetest mudel õigesti ennustab\n",
        "for i in range(len(pred)):\n",
        "    predicted_label = np.argmax(pred[i])\n",
        "    actual_label = np.argmax(y_test[i])\n",
        "    print(f\"Ennustus: {predicted_label}\")\n",
        "    print(f\"Tegelik: {actual_label}\")\n",
        "    if predicted_label == actual_label:\n",
        "        correct += 1\n",
        "    print()\n",
        "\n",
        "accuracy = correct / len(pred)\n",
        "print(f\"Täpsus: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLzmLOwmuL79",
        "outputId": "05c217e5-a186-4c21-cc96-b52d24654fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Number of test samples: 100\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 6\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 6\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 6\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Overall accuracy: 17.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mudeli testimine väiksematel andmetele"
      ],
      "metadata": {
        "id": "1cMt3IwjqYU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Väiksemaks testimiseks võetakse 3 liiki\n",
        "liigid = [\"asbflytest\", \"blakit1test\", \"categrtest\"]"
      ],
      "metadata": {
        "id": "_nwzcC7dkFz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kõigi failide sisse lugemine ja labeli numbri lisamine\n",
        "files = []\n",
        "for i in range(len(liigid)):\n",
        "  liik = liigid[i]\n",
        "  liigi_failid = glob.glob(liik+\"/*.ogg\")\n",
        "  for f in liigi_failid:\n",
        "    files.append((f, i))\n",
        "# Ajan järjendis heliklipid segamine, et edaspidi oleks lihtsam treening ja testandmeteks jagada\n",
        "np.random.shuffle(files)"
      ],
      "metadata": {
        "id": "7oiFu6UZrDdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Andmete eeltöötlus\n",
        "X, y = process_files(files)\n",
        "#Klasside numbrid dünaamiliselt ja traintest suuruse määramine\n",
        "num_of_classes = len(np.unique(y)) + 1\n",
        "traintest_size = 0.2\n",
        "\n",
        "# Ennustava atribuudi one-hot encodimine, et seda anda mudelile ette\n",
        "y = to_categorical(y, num_classes=num_of_classes)\n",
        "\n",
        "# Probleem: andmed on arrayde arrayd, flattenib iga array X-is, et sobiks standarscalerile\n",
        "# Teeb listi 2D arrayks\n",
        "X_flattened = np.array([item.flatten() for item in X])\n",
        "\n",
        "# StandardScaleme\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_flattened)\n",
        "\n",
        "# Rehsape tagasi\n",
        "feature_count = X[0].shape[0]\n",
        "time_steps = X[0].shape[1]\n",
        "X_scaled = X_scaled.reshape(len(X_scaled), feature_count, time_steps, 1)\n",
        "\n",
        "# Train test split\n",
        "test_len = int(len(X_scaled) * traintest_size)\n",
        "X_train = X_scaled[test_len:]\n",
        "y_train = y[test_len:]\n",
        "X_test = X_scaled[:test_len]\n",
        "y_test = y[:test_len]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc70815-e4ed-4551-9883-dca79071dbf2",
        "id": "N-Q73tnOrHM2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 421ms/step - accuracy: 0.0956 - loss: 2.8398\n",
            "Epoch 2/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1065 - loss: 2.3926\n",
            "Epoch 3/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1040 - loss: 2.3978\n",
            "Epoch 4/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1026 - loss: 2.3425\n",
            "Epoch 5/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1360 - loss: 2.2999\n",
            "Epoch 6/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1142 - loss: 2.3292\n",
            "Epoch 7/7\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1452 - loss: 2.2493\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7add5d799c30>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mudeli loomine\n",
        "model2 = Sequential([\n",
        "    Input(shape=(feature_count, time_steps, 1)),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "    Flatten(),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_of_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile ja fit\n",
        "model2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model2.summary()\n",
        "model2.fit(X_train, y_train, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "id": "aEtlGrc4rNsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mudeli testimine testandmetel\n",
        "pred = model.predict(X_test)\n",
        "correct = 0\n",
        "\n",
        "print(f\"Testandmeid on {len(X_test)}.\")\n",
        "# Vaadatakse palju testandmetest mudel õigesti ennustab\n",
        "for i in range(len(pred)):\n",
        "    predicted_label = np.argmax(pred[i])\n",
        "    actual_label = np.argmax(y_test[i])\n",
        "    print(f\"Ennustus: {predicted_label}\")\n",
        "    print(f\"Tegelik: {actual_label}\")\n",
        "    if predicted_label == actual_label:\n",
        "        correct += 1\n",
        "    print()\n",
        "\n",
        "accuracy = correct / len(pred)\n",
        "print(f\"Täpsus: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c217e5-a186-4c21-cc96-b52d24654fa5",
        "id": "WSsTwOSHrT-N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "Number of test samples: 100\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 6\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 6\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 6\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 0\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 2\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 5\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 7\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 9\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 1\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 4\n",
            "\n",
            "Prediction: 0\n",
            "Actually: 3\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 8\n",
            "\n",
            "Prediction: 1\n",
            "Actually: 2\n",
            "\n",
            "Overall accuracy: 17.00%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "NTZgtTzZ4H32",
        "CS1GgnT5qUiD",
        "1cMt3IwjqYU7"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}